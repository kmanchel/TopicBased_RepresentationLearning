{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import official.nlp.bert.tokenization as bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/u40332/NLP/data/bias/cleanish_lower_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 248  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenizer.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = df.content.iloc[1]\n",
    "len(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0417 19:25:48.548464 140420488449792 file_utils.py:57] TensorFlow version 2.2.0-dev20200417 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0417 19:26:02.912541 140420488449792 filelock.py:274] Lock 140414832718792 acquired on /home/u40332/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "I0417 19:26:02.916854 140420488449792 file_utils.py:479] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /home/u40332/.cache/torch/transformers/tmphs32pdh1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09170b1504cc4c71a5a0cbdbfaf4e42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0417 19:26:03.716160 140420488449792 file_utils.py:489] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /home/u40332/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0417 19:26:03.729353 140420488449792 file_utils.py:492] creating metadata file for /home/u40332/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0417 19:26:03.744686 140420488449792 filelock.py:318] Lock 140414832718792 released on /home/u40332/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "I0417 19:26:03.746164 140420488449792 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/u40332/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald trump ran on many braggadocios and largely unrealistic campaign promises. one of those promises was to be the best the hugest the most competent infrastructure president the united states has ever seen. trump was going to fix every infrastructure problem in the country and make america great again in the process. that is unless youre a brown american. in that case youre on your own even after a massive natural disaster like hurricane maria. puerto ricos debt which the puerto rican citizens not in government would have no responsibility for has nothing to do with using federal emergency disaster funds to save the lives of american citizens there. the infrastructure is certainly a mess at this point after a category 5 hurricane ripped through the island and 84 percent of puerto rican people are currently without electricity. emergency efforts after hurricanes irma and harvey reportedly went very well and trump praised himself as well and even saw his disastrous approval ratings tick up slightly as a result. however the insufficient response in puerto rico has nothing to do with trump in his mind and can only be blamed on the people there who do not live in a red state and have no electoral college votes to offer the new president for 2020. theyre on their own. twitter responded with sheer incredulity at trumps vicious attack on an already suffering people. featured image screengrab via youtube\n",
      "[101, 6221, 8398, 2743, 2006, 2116, 23678, 9365, 9793, 2015, 1998, 4321, 4895, 22852, 6553, 3049, 10659, 1012, 2028, 1997, 2216, 10659, 2001, 2000, 2022, 1996, 2190, 1996, 4121, 3367, 1996, 2087, 17824, 6502, 2343, 1996, 2142, 2163, 2038, 2412, 2464, 1012, 8398, 2001, 2183, 2000, 8081, 2296, 6502, 3291, 1999, 1996, 2406, 1998, 2191, 2637, 2307, 2153, 1999, 1996, 2832, 1012, 2008, 2003, 4983, 2115, 2063, 1037, 2829, 2137, 1012, 1999, 2008, 2553, 2115, 2063, 2006, 2115, 2219, 2130, 2044, 1037, 5294, 3019, 7071, 2066, 7064, 3814, 1012, 5984, 7043, 2015, 7016, 2029, 1996, 5984, 13641, 4480, 2025, 1999, 2231, 2052, 2031, 2053, 5368, 2005, 2038, 2498, 2000, 2079, 2007, 2478, 2976, 5057, 7071, 5029, 2000, 3828, 1996, 3268, 1997, 2137, 4480, 2045, 1012, 1996, 6502, 2003, 5121, 1037, 6752, 2012, 2023, 2391, 2044, 1037, 4696, 1019, 7064, 9157, 2083, 1996, 2479, 1998, 6391, 3867, 1997, 5984, 13641, 2111, 2024, 2747, 2302, 6451, 1012, 5057, 4073, 2044, 17035, 20868, 2863, 1998, 7702, 7283, 2253, 2200, 2092, 1998, 8398, 5868, 2370, 2004, 2092, 1998, 2130, 2387, 2010, 16775, 6226, 8599, 16356, 2039, 3621, 2004, 1037, 2765, 1012, 2174, 1996, 13990, 3433, 1999, 5984, 7043, 2038, 2498, 2000, 2079, 2007, 8398, 1999, 2010, 2568, 1998, 2064, 2069, 2022, 11248, 2006, 1996, 2111, 2045, 2040, 2079, 2025, 2444, 1999, 1037, 2417, 2110, 1998, 2031, 2053, 6092, 2267, 4494, 2000, 3749, 1996, 2047, 2343, 2005, 12609, 1012, 2027, 2890, 2006, 2037, 2219, 1012, 10474, 5838, 2007, 11591, 4297, 5596, 15859, 3723, 2012, 8398, 2015, 13925, 2886, 2006, 2019, 2525, 6114, 2111, 1012, 2956, 3746, 3898, 17643, 2497, 3081, 7858, 102]\n"
     ]
    }
   ],
   "source": [
    "print(sent)\n",
    "print(tokenizer.encode(sent,add_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
